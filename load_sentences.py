#!/usr/bin/env python3
"""
Script simple para cargar sentencias al servidor Rails.
Puede procesar archivos existentes o scraping de per√≠odos espec√≠ficos.
"""

import asyncio
import os
import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
import requests

from sentence_html_ingestor import SentenceHTMLIngestor

# Configuraci√≥n
API_URL = "https://lexia.uy/v1/sentences"
BASE_DATA_PATH = Path(os.getenv("LEXIA_BRAIN_DATA_PATH", "./data"))
SENTENCES_HTML_DIR = BASE_DATA_PATH / "sentences" / "html"
PROCESSED_JSON_DIR = BASE_DATA_PATH / "sentences" / "json"

# Configuraci√≥n de chunking (compatible con sentence_processor.py)
MAX_TOKENS_CHUNKING = 512
OVERLAP = 0.20

def format_sentence_for_rails(sentence_data: Dict[str, Any]) -> Dict[str, Any]:
    """Formatea los datos para Rails."""
    return {
        "number": sentence_data.get("number"),
        "court": sentence_data.get("court"),
        "importance": sentence_data.get("importance"),
        "sentence_type": sentence_data.get("sentence_type"),
        "date": sentence_data.get("date"),
        "file_number": sentence_data.get("file_number"),
        "procedure": sentence_data.get("procedure"),
        "subjects": sentence_data.get("subjects", []),
        "summary": sentence_data.get("summary"),
        "text": sentence_data.get("text"),
        "raw_text": sentence_data.get("raw_text"),
        "signatories": sentence_data.get("signatories", []),
        "discordants": sentence_data.get("discordants", []),
        "editors": sentence_data.get("editors", []),
        "descriptors": sentence_data.get("descriptors", []),
        "short_embeddings_attributes": sentence_data.get("short_embeddings_attributes", []),
        "long_embeddings_attributes": sentence_data.get("long_embeddings_attributes", [])
    }

def send_sentence(sentence_data: Dict[str, Any]) -> bool:
    """Env√≠a una sentencia a la API."""
    payload = {"sentence": format_sentence_for_rails(sentence_data)}
    headers = {"Content-Type": "application/json"}
    
    sentence_id = sentence_data.get("id", "unknown")
    print(f"Enviando sentencia {sentence_id}...")
    
    try:
        response = requests.post(API_URL, json=payload, headers=headers, timeout=30)
        
        if response.ok:
            print(f"‚úì Sentencia {sentence_id} enviada exitosamente")
            return True
        else:
            print(f"‚úó Error {response.status_code}: {response.text}")
            return False
            
    except Exception as e:
        print(f"‚úó Error de conexi√≥n: {e}")
        return False

async def scrape_sentences_for_period(
    start_date: datetime, end_date: datetime, output_dir: Path
) -> List[Path]:
    """
    Scraping de sentencias para un per√≠odo espec√≠fico.
    Compatible con sentence_processor.py
    """
    print(f"üîç Iniciando scraping para per√≠odo: {start_date.strftime('%d/%m/%Y')} a {end_date.strftime('%d/%m/%Y')}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # Importar scraper solo cuando sea necesario
        from scraper import sentences_scraper
        
        await sentences_scraper.scrape_sentences(
            session_start_date=start_date,
            session_end_date=end_date,
            output_dir=output_dir,
        )
        
        scraped_files = list(output_dir.glob("*.html"))
        print(f"‚úì Scraping completado. Encontrados {len(scraped_files)} archivos")
        return scraped_files
        
    except ImportError:
        print("‚ö† M√≥dulo 'scraper' no disponible. Solo se procesar√°n archivos existentes.")
        return []
    except Exception as e:
        print(f"‚úó Error durante scraping: {e}")
        return []

def process_existing_files(html_dir: Path) -> List[Path]:
    """Procesa archivos HTML existentes en el directorio."""
    if not html_dir.exists():
        print(f"‚úó Directorio no existe: {html_dir}")
        return []
    
    html_files = list(html_dir.glob("*.html"))
    if not html_files:
        print("No se encontraron archivos HTML")
        return []
    
    print(f"Encontrados {len(html_files)} archivos HTML existentes")
    return html_files

async def load_sentences_for_period(
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None,
    process_existing: bool = True
):
    """
    Carga sentencias para un per√≠odo espec√≠fico o procesa archivos existentes.
    
    Args:
        start_date: Fecha de inicio para scraping (opcional)
        end_date: Fecha de fin para scraping (opcional)
        process_existing: Si procesar archivos existentes en el directorio
    """
    print("üöÄ Iniciando carga de sentencias al servidor Rails")
    print(f"API: {API_URL}")
    print(f"Directorio: {SENTENCES_HTML_DIR}")
    
    # Crear directorios necesarios
    SENTENCES_HTML_DIR.mkdir(parents=True, exist_ok=True)
    PROCESSED_JSON_DIR.mkdir(parents=True, exist_ok=True)
    
    # Crear ingestor CON embeddings
    ingestor = SentenceHTMLIngestor(
        chunking_params={
            "max_tokens": MAX_TOKENS_CHUNKING,
            "overlap": OVERLAP
        },
        force_chunking=True  # Activar embeddings
    )
    
    html_files = []
    
    # Scraping de per√≠odo espec√≠fico si se proporcionan fechas
    if start_date and end_date:
        scraped_files = await scrape_sentences_for_period(
            start_date, end_date, SENTENCES_HTML_DIR
        )
        html_files.extend(scraped_files)
    
    # Procesar archivos existentes si se solicita
    if process_existing:
        existing_files = process_existing_files(SENTENCES_HTML_DIR)
        html_files.extend(existing_files)
    
    if not html_files:
        print("No hay archivos para procesar")
        return
    
    # Eliminar duplicados manteniendo el orden
    html_files = list(dict.fromkeys(html_files))
    print(f"Total de archivos a procesar: {len(html_files)}")
    
    # Procesar archivos
    successful = 0
    failed = 0
    
    for html_file in html_files:
        print(f"\nProcesando: {html_file.name}")
        
        try:
            # Procesar HTML
            sentence_data = ingestor.ingest_file(html_file)
            
            if not sentence_data:
                print(f"‚úó No se pudo procesar {html_file.name}")
                failed += 1
                continue
            
            # Validar campos requeridos
            if not sentence_data.get("number") or not sentence_data.get("court"):
                print(f"‚úó Faltan campos requeridos en {html_file.name}")
                failed += 1
                continue
            
            # Enviar a API
            if send_sentence(sentence_data):
                successful += 1
                # Guardar JSON y eliminar HTML
                try:
                    # Guardar datos procesados como JSON
                    json_filename = html_file.stem + ".json"
                    json_path = PROCESSED_JSON_DIR / json_filename
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(sentence_data, f, ensure_ascii=False, indent=2, default=str)
                    print(f"‚úì JSON guardado: {json_filename}")
                    
                    # Eliminar archivo HTML original
                    html_file.unlink()
                    print(f"‚úì HTML eliminado: {html_file.name}")
                except Exception as e:
                    print(f"‚ö† Error al guardar JSON o eliminar HTML: {e}")
            else:
                failed += 1
                
        except Exception as e:
            print(f"‚úó Error procesando {html_file.name}: {e}")
            failed += 1
    
    # Resumen
    print(f"\n{'='*50}")
    print(f"RESUMEN DE PROCESAMIENTO")
    print(f"{'='*50}")
    print(f"Exitosos: {successful}")
    print(f"Fallidos: {failed}")
    print(f"Total: {successful + failed}")
    if successful + failed > 0:
        success_rate = (successful / (successful + failed)) * 100
        print(f"Tasa de √©xito: {success_rate:.1f}%")

def main():
    """Funci√≥n principal con opciones de l√≠nea de comandos."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Cargar sentencias al servidor Rails")
    parser.add_argument(
        "--start-date", 
        type=str, 
        help="Fecha de inicio para scraping (formato: DD/MM/YYYY)"
    )
    parser.add_argument(
        "--end-date", 
        type=str, 
        help="Fecha de fin para scraping (formato: DD/MM/YYYY)"
    )
    parser.add_argument(
        "--no-existing", 
        action="store_true", 
        help="No procesar archivos existentes, solo hacer scraping"
    )
    
    args = parser.parse_args()
    
    start_date = None
    end_date = None
    
    # Parsear fechas si se proporcionan
    if args.start_date:
        try:
            start_date = datetime.strptime(args.start_date, "%d/%m/%Y")
        except ValueError:
            print("‚úó Formato de fecha de inicio inv√°lido. Use DD/MM/YYYY")
            return
    
    if args.end_date:
        try:
            end_date = datetime.strptime(args.end_date, "%d/%m/%Y")
        except ValueError:
            print("‚úó Formato de fecha de fin inv√°lido. Use DD/MM/YYYY")
            return
    
    # Validar que ambas fechas est√©n presentes si se usa scraping
    if (start_date and not end_date) or (end_date and not start_date):
        print("‚úó Debe proporcionar tanto fecha de inicio como de fin para scraping")
        return
    
    if start_date and end_date and start_date > end_date:
        print("‚úó La fecha de inicio debe ser anterior a la fecha de fin")
        return
    
    process_existing = not args.no_existing
    
    # Ejecutar procesamiento
    try:
        asyncio.run(load_sentences_for_period(
            start_date=start_date,
            end_date=end_date,
            process_existing=process_existing
        ))
    except KeyboardInterrupt:
        print("\n‚ö† Interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")

if __name__ == "__main__":
    main()